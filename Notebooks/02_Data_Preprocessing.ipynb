{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb8d3dcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/sumz/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# ═══════════════════════════════════════════════\n",
    "# 1. Librerías estándar\n",
    "# ═══════════════════════════════════════════════\n",
    "import os\n",
    "import sys\n",
    "import ast\n",
    "import json\n",
    "from collections import Counter\n",
    "\n",
    "# ═══════════════════════════════════════════════\n",
    "# 2. Ciencia de datos y utilidades\n",
    "# ═══════════════════════════════════════════════\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# ═══════════════════════════════════════════════\n",
    "# 3. Preprocesamiento y modelos de scikit-learn\n",
    "# ═══════════════════════════════════════════════\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import (\n",
    "    OneHotEncoder,\n",
    "    StandardScaler,\n",
    "    MultiLabelBinarizer\n",
    ")\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# ═══════════════════════════════════════════════\n",
    "# 4. Modelos y embeddings\n",
    "# ═══════════════════════════════════════════════\n",
    "from xgboost import XGBClassifier\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# ═══════════════════════════════════════════════\n",
    "# 5. Funciones personalizadas del proyecto\n",
    "# ═══════════════════════════════════════════════\n",
    "from new_or_used import build_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48b5fb9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_raw, y_train, X_test_raw, y_test = build_dataset()\n",
    "X_train = pd.json_normalize(X_train_raw)\n",
    "X_test = pd.json_normalize(X_test_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd9a4a7",
   "metadata": {},
   "source": [
    "# Process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d682c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(df, is_train=True, mlb_tags=None, model=None, kmeans=None, cluster_map=None):\n",
    "    df = df.copy()\n",
    "\n",
    "    df.drop(columns=['pictures'], inplace=True)\n",
    "    \n",
    "    # === 1. Parsear descripción\n",
    "    def parse_description(cell):\n",
    "        if isinstance(cell, list) and len(cell) > 0:\n",
    "            try:\n",
    "                return ast.literal_eval(cell[0])\n",
    "            except:\n",
    "                return None\n",
    "        return None\n",
    "\n",
    "    df['descriptions'] = df['descriptions'].apply(parse_description)\n",
    "    df['description_id'] = df['descriptions'].apply(lambda x: x.get('id') if isinstance(x, dict) else None)\n",
    "    df.drop(columns=['descriptions'], inplace=True)\n",
    "\n",
    "    # === 2. Procesar métodos de pago\n",
    "    df['payment_descriptions'] = df['non_mercado_pago_payment_methods'].apply(\n",
    "        lambda lst: [d['description'] for d in lst] if isinstance(lst, list) else []\n",
    "    )\n",
    "    mlb = MultiLabelBinarizer()\n",
    "    payment_dummies = pd.DataFrame(\n",
    "        mlb.fit_transform(df['payment_descriptions']),\n",
    "        columns=[f\"non_mercado_pago_payment_methods_description_{desc}\" for desc in mlb.classes_],\n",
    "        index=df.index\n",
    "    )\n",
    "    df = pd.concat([df, payment_dummies], axis=1)\n",
    "\n",
    "    # Agrupar métodos de pago relacionados con tarjeta\n",
    "    tarjetas = [\"Visa\", \"MasterCard\", \"Visa Electron\", \"Mastercard Maestro\", \"American Express\", \"Diners\", \"Tarjeta de crédito\"]\n",
    "    tarjetas_cols = [\n",
    "        f\"non_mercado_pago_payment_methods_description_{t}\"\n",
    "        for t in tarjetas if f\"non_mercado_pago_payment_methods_description_{t}\" in df.columns\n",
    "    ]\n",
    "    df[\"payment_method_Tarjeta\"] = df[tarjetas_cols].sum(axis=1).clip(upper=1)\n",
    "    df.drop(columns=['non_mercado_pago_payment_methods'] + tarjetas_cols + ['payment_descriptions'], inplace=True)\n",
    "\n",
    "    # === 3. Columnas binarias a partir de listas\n",
    "    df['has_variations'] = df['variations'].apply(lambda x: isinstance(x, list) and len(x) > 0)\n",
    "    df['has_attributes'] = df['attributes'].apply(lambda x: isinstance(x, list) and len(x) > 0)\n",
    "    df.drop(columns=['sub_status', 'coverage_areas', 'variations', 'deal_ids', 'attributes', 'shipping.methods', 'shipping.tags'], inplace=True)\n",
    "\n",
    "    # === 4. Tags\n",
    "    tags_list = df['tags'].apply(lambda x: x if isinstance(x, list) else [])\n",
    "    if is_train:\n",
    "        mlb_tags = MultiLabelBinarizer()\n",
    "        tags_encoded = pd.DataFrame(mlb_tags.fit_transform(tags_list), columns=[f'tag_{t}' for t in mlb_tags.classes_], index=df.index)\n",
    "    else:\n",
    "        tags_encoded = pd.DataFrame(mlb_tags.transform(tags_list), columns=[f'tag_{t}' for t in mlb_tags.classes_], index=df.index)\n",
    "    df = pd.concat([df, tags_encoded], axis=1)\n",
    "    df.drop(columns=['tags'], inplace=True)\n",
    "\n",
    "    # === 5. Limpieza general de nulos y strings vacíos\n",
    "    df.replace(['none', 'None', ''], np.nan, inplace=True)\n",
    "    df = df.applymap(lambda x: np.nan if x is None else x)\n",
    "\n",
    "    # === 6. Eliminar columnas con demasiados nulos o con correlaciones altas con otras variables \n",
    "    drop_cols = [\n",
    "        'listing_source', 'international_delivery_mode', 'official_store_id', 'differential_pricing', 'original_price',\n",
    "        'video_id', 'catalog_product_id', 'subtitle', 'shipping.dimensions', 'shipping.free_methods',\n",
    "        'last_updated', 'id', 'thumbnail', 'title', 'secure_thumbnail', 'permalink',\n",
    "        'seller_address.city.id', 'parent_item_id', 'seller_address.state.id', 'base_price', 'initial_quantity', 'site_id', 'seller_address.country.name', 'seller_address.country.id', 'description_id'\n",
    "    ]\n",
    "    df.drop(columns=[col for col in drop_cols if col in df.columns], inplace=True)\n",
    "\n",
    "    # === 7. Conversión de booleanos\n",
    "    bool_cols = df.select_dtypes(include='bool').columns.tolist()\n",
    "    df[bool_cols] = df[bool_cols].astype(float)\n",
    "\n",
    "    # === 8. Procesamiento de 'warranty' con embeddings\n",
    "    if is_train:\n",
    "        model = SentenceTransformer('sentence-transformers/all-MiniLM-L12-v2')\n",
    "        texts = df['warranty'].dropna().unique()\n",
    "        embeddings = model.encode(texts, show_progress_bar=False)\n",
    "        kmeans = KMeans(n_clusters=4, random_state=42)\n",
    "        clusters = kmeans.fit_predict(embeddings)\n",
    "        cluster_map = dict(zip(texts, clusters))\n",
    "\n",
    "    df['warranty_cluster'] = df['warranty'].map(cluster_map).fillna(-1)\n",
    "    mapeo_final = {0: \"tiene_garantia\", 1: \"tiene_garantia\", 2: \"tiene_garantia\", 3: \"no_menciona\", -1: \"no_menciona\"}\n",
    "    df['warranty_label'] = df['warranty_cluster'].map(mapeo_final)\n",
    "\n",
    "    def corregir_warranty(texto, etiqueta_inicial):\n",
    "        if pd.isna(texto):\n",
    "            return etiqueta_inicial\n",
    "        texto_limpio = texto.lower()\n",
    "        if \"sin garantia\" in texto_limpio or \"sin garantía\" in texto_limpio:\n",
    "            return \"sin_garantia\"\n",
    "        return etiqueta_inicial\n",
    "\n",
    "    df['warranty_label'] = df.apply(lambda row: corregir_warranty(row['warranty'], row['warranty_label']), axis=1)\n",
    "    df['has_warranty'] = (df['warranty_label'] == 'tiene_garantia').astype(float)\n",
    "    df.drop(columns=['warranty', 'warranty_cluster', 'warranty_label'], inplace=True)\n",
    "\n",
    "    # === 9. Procesamiento de fechas\n",
    "    df['date_created'] = pd.to_datetime(df['date_created'], errors='coerce')\n",
    "    df['year'] = df['date_created'].dt.year\n",
    "    df['month'] = df['date_created'].dt.month\n",
    "    df['day'] = df['date_created'].dt.day\n",
    "    df['weekday'] = df['date_created'].dt.weekday\n",
    "    df['is_weekend'] = df['weekday'].isin([5, 6]).astype(int)\n",
    "    df.drop(columns=['date_created'], inplace=True)\n",
    "\n",
    "    df[\"start_time\"] = pd.to_datetime(df[\"start_time\"], errors=\"coerce\", unit=\"ms\")\n",
    "    df[\"stop_time\"] = pd.to_datetime(df[\"stop_time\"], errors=\"coerce\", unit=\"ms\")\n",
    "    df[\"active_period\"] = (df['stop_time'] - df['start_time']).dt.days\n",
    "    df.drop(columns=[\"start_time\", \"stop_time\"], inplace=True)\n",
    "\n",
    "\n",
    "    # === 10. Ajustes finales\n",
    "    df[\"Currency_ARS\"] = (df[\"currency_id\"] == \"ARS\").astype(int)\n",
    "    df.drop(columns=[\"currency_id\"], inplace=True)\n",
    "\n",
    "    df[\"status\"] = df[\"status\"].apply(lambda x: x if x in [\"active\", \"paused\"] else \"other\")\n",
    "    df[\"shipping.mode\"] = df[\"shipping.mode\"].replace(\"me1\", \"custom\")\n",
    "\n",
    "    # === 11. Eliminar filas con NaN finales\n",
    "    if is_train:\n",
    "        df = df.dropna(axis=0)\n",
    "\n",
    "    # === 12. Guardar artefactos si es entrenamiento\n",
    "    if is_train:\n",
    "        os.makedirs(\"../artifacts/preprocessors\", exist_ok=True)\n",
    "        joblib.dump(kmeans, \"../artifacts/preprocessors/kmeans_warranty.pkl\")\n",
    "        joblib.dump(cluster_map, \"../artifacts/preprocessors/warranty_cluster_map.pkl\")\n",
    "        joblib.dump(mlb_tags, \"../artifacts/preprocessors/mlb_tags.pkl\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5537d019",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/83/lhxwld796l7cx3lyt07ml50m0000gn/T/ipykernel_4725/3017475250.py:65: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df.replace(['none', 'None', ''], np.nan, inplace=True)\n",
      "/var/folders/83/lhxwld796l7cx3lyt07ml50m0000gn/T/ipykernel_4725/3017475250.py:66: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df = df.applymap(lambda x: np.nan if x is None else x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(89998, 39)\n",
      "Modelos cargados.\n",
      "Preprocesamiento de X_test completado.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/83/lhxwld796l7cx3lyt07ml50m0000gn/T/ipykernel_4725/3017475250.py:65: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df.replace(['none', 'None', ''], np.nan, inplace=True)\n",
      "/var/folders/83/lhxwld796l7cx3lyt07ml50m0000gn/T/ipykernel_4725/3017475250.py:66: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df = df.applymap(lambda x: np.nan if x is None else x)\n"
     ]
    }
   ],
   "source": [
    "# Para entrenamiento\n",
    "X_train = preprocess(X_train, is_train=True)\n",
    "print(X_train.shape)\n",
    "\n",
    "# Para test, cargando los modelos previamente guardados\n",
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L12-v2')\n",
    "kmeans = joblib.load(\"../artifacts/preprocessors/kmeans_warranty.pkl\")\n",
    "cluster_map = joblib.load(\"../artifacts/preprocessors/warranty_cluster_map.pkl\")\n",
    "mlb_tags = joblib.load(\"../artifacts/preprocessors/mlb_tags.pkl\")\n",
    "\n",
    "print(\"Modelos cargados.\")\n",
    "\n",
    "X_test = preprocess(X_test, is_train=False, model=model, kmeans=kmeans, cluster_map=cluster_map, mlb_tags=mlb_tags)\n",
    "\n",
    "print(\"Preprocesamiento de X_test completado.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86379364",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = X_train['condition']\n",
    "y_test = pd.Series(y_test)\n",
    "X_train.drop(columns=['condition'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e94fb347",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets guardados en 'Data/Gold/'\n"
     ]
    }
   ],
   "source": [
    "# Crear la carpeta si no existe\n",
    "os.makedirs(\"../Data/Gold\", exist_ok=True)\n",
    "\n",
    "# Guardar datasets\n",
    "X_train.to_csv(\"../Data/Gold/X_train_gold.csv\", index=False)\n",
    "X_test.to_csv(\"../Data/Gold/X_test_gold.csv\", index=False)\n",
    "y_train.to_csv(\"../Data/Gold/y_train_gold.csv\", index=False)\n",
    "y_test.to_csv(\"../Data/Gold/y_test_gold.csv\", index=False)\n",
    "\n",
    "\n",
    "with open(\"../Data/bronce/X_test_raw.jsonlines\", \"w\") as f:\n",
    "    for item in X_test_raw:\n",
    "        f.write(json.dumps(item) + \"\\n\")\n",
    "\n",
    "\n",
    "print(\"Datasets guardados en 'Data/Gold/'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sumz",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
